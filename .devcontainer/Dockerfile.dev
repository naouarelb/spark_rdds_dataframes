FROM ubuntu:latest

ENV DEBIAN_FRONTEND=noninteractive

# Update, upgrade, and install required packages safely
RUN apt-get update \
    && apt-get upgrade -y \
    && apt-get install -y --no-install-recommends \
        curl \
        build-essential \
        python3-pip \
        python3 \
        git \
        unzip \
        sudo \
        openjdk-11-jdk \
        vim \
    && apt-get autoremove -y \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*

# Install Scala separately from official repo (safer)
RUN curl -fSL https://downloads.lightbend.com/scala/2.13.12/scala-2.13.12.deb -o scala.deb \
    && dpkg -i scala.deb \
    && rm scala.deb

# Install Apache Spark
RUN curl -O https://archive.apache.org/dist/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz \
    && tar xvf spark-3.2.0-bin-hadoop3.2.tgz \
    && mkdir -p /opt/spark \
    && mv spark-3.2.0-bin-hadoop3.2/* /opt/spark \
    && chmod -R 777 /opt/spark \
    && rm -rf spark-3.2.0-bin-hadoop3.2.tgz spark-3.2.0-bin-hadoop3.2

RUN apt-get update \
    && apt-get install -y --no-install-recommends software-properties-common \
    && add-apt-repository ppa:deadsnakes/ppa \
    && apt-get update \
    && apt-get install -y --no-install-recommends \
        python3.10 \
        python3.10-distutils \
        python3.10-venv \
        python3-pip \
    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.10 1 \
    && rm -rf /var/lib/apt/lists/* \
    && apt-get autoremove -y \
    && apt-get clean

RUN pip install uv

# Set Spark environment
ENV SPARK_HOME=/opt/spark
ENV PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin

# Verify installations
RUN java -version \
    python3 --version \
    && scala -version \
    && spark-submit --version \
    && uv --version


WORKDIR /workspace
